{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"ontario-tech-univ-logo.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image stitching\n",
    "\n",
    "Faisal Qureshi   \n",
    "Professor    \n",
    "Faculty of Science    \n",
    "Ontario Tech University    \n",
    "Oshawa ON Canada    \n",
    "http://vclab.science.ontariotechu.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright information\n",
    "¬© Faisal Qureshi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution-NonCommercial 4.0 International License.](https://creativecommons.org/licenses/by-nc/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "Consider the following image pair\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "    <td>\n",
    "    <img src=\"1-left.jpeg\" width=\"30%\"></img>\n",
    "    <img src=\"1-right.jpeg\" width=\"30%\"></img>\n",
    "    </td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "You are asked to pick at least four correspondences in both images. E.g., you can pick four points  $(ùë•^ùëô_{ùëñ},ùë¶^ùëô_{ùëñ})$  in the left image and then you can pick the corresponding locations  $(ùë•^ùëü_{ùëñ},ùë¶^ùëü{ùëñ})$  in the right image in the same order. Here  ùëñ > 3 . Given these correspondences, estimate the homography matrix between the two set of images, and use this matrix to stitch the two images into a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking locations in an image\n",
    "Use the following code to pick locations on an image as seen below\n",
    "\n",
    "![Paper written in LaTeX](image.png)\n",
    "\n",
    "Modify the code as needed to record the  (ùë•,ùë¶)  locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointPicker:\n",
    "    points_selected = []\n",
    "    \n",
    "    def __init__(self, imgplot, img, color=(255,0,0), radius=20, thickness=20):\n",
    "        self.imgplot = imgplot\n",
    "        self.img = img\n",
    "        self.color = color\n",
    "        self.radius = radius\n",
    "        self.thickness = thickness\n",
    "        self.cid = imgplot.figure.canvas.mpl_connect('button_press_event', self)\n",
    "    \n",
    "    def __call__(self, event):\n",
    "        if event.inaxes != self.imgplot.axes: \n",
    "            return\n",
    "        ix = event.xdata\n",
    "        iy = event.ydata\n",
    "        print(f'x={ix}, y={iy}')\n",
    "        self.img = cv.circle(self.img, (int(ix), int(iy)), self.radius, self.color, self.thickness)\n",
    "        \n",
    "        imgplot.set_array(self.img)\n",
    "        self.imgplot.figure.canvas.draw()\n",
    "    \n",
    "    def clear_points(self):\n",
    "        self.points_selected.clear()\n",
    "\n",
    "img=cv.imread('data/1-left.jpeg')        \n",
    "        \n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "imgplot = plt.imshow(img)\n",
    "point_picker = PointPicker(imgplot, img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_descriptors(img1, img2):\n",
    "    akaze = cv.AKAZE_create()\n",
    "    kp1, des1 = akaze.detectAndCompute(img1, None)\n",
    "    kp2, des2 = akaze.detectAndCompute(img2, None)\n",
    "    \n",
    "    matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE_HAMMING)\n",
    "    nn_matches = matcher.knnMatch(des1, des2, 2)\n",
    "    \n",
    "    good = []\n",
    "    src = []\n",
    "    dst = []\n",
    "    for match in nn_matches:\n",
    "        if len(match) < 2: continue\n",
    "        m, n = match[0], match[1]\n",
    "        if m.distance < 0.8*n.distance:\n",
    "            good.append([m])\n",
    "            src.append(kp1[m.queryIdx].pt)\n",
    "            dst.append(kp2[n.trainIdx].pt)\n",
    "\n",
    "    result_img = cv.drawMatchesKnn(img1,kp1,img2,kp2, good, None, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "    return np.float32(src), np.float32(dst), result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_img = cv.imread(\"data/1-right.jpeg\") \n",
    "left_img = cv.imread(\"data/1-left.jpeg\")\n",
    "\n",
    "src, dst, result_img = match_descriptors(right_img, left_img)\n",
    "    \n",
    "\n",
    "H, masked = cv.findHomography(src, dst, cv.RANSAC, 5.0)\n",
    "\n",
    "dst = cv.warpPerspective(right_img,H,(left_img.shape[1] + right_img.shape[1], left_img.shape[0]))\n",
    "dst[0:left_img.shape[0], 0:left_img.shape[1]] = left_img\n",
    "cv.imwrite(\"data/1-stitched.jpg\",dst)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(dst)\n",
    "plt.show()\n",
    "\n",
    "#This didn't yield desired results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_img = cv.imread(\"data/2-right.jpeg\") \n",
    "left_img = cv.imread(\"data/2-left.jpeg\")\n",
    "\n",
    "src, dst, result_img = match_descriptors(right_img, left_img)\n",
    "    \n",
    "\n",
    "H, masked = cv.findHomography(src, dst, cv.RANSAC, 5.0)\n",
    "\n",
    "dst = cv.warpPerspective(right_img,H,(left_img.shape[1] + right_img.shape[1], left_img.shape[0]))\n",
    "dst[0:left_img.shape[0], 0:left_img.shape[1]] = left_img\n",
    "cv.imwrite(\"data/2-stitched.jpg\",dst)\n",
    "plt.imshow(dst)\n",
    "plt.imshow()\n",
    "\n",
    "#This did yield desired results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_img = cv.imread(\"data/3-right.jpeg\") \n",
    "left_img = cv.imread(\"data/3-left.jpeg\")\n",
    "\n",
    "src, dst, result_img = match_descriptors(right_img, left_img)\n",
    "    \n",
    "\n",
    "H, masked = cv.findHomography(src, dst, cv.RANSAC, 5.0)\n",
    "\n",
    "dst = cv.warpPerspective(right_img,H,(left_img.shape[1] + right_img.shape[1], left_img.shape[0]))\n",
    "dst[0:left_img.shape[0], 0:left_img.shape[1]] = left_img\n",
    "cv.imwrite(\"data/3-stitched.jpg\",dst)\n",
    "plt.imshow(dst)\n",
    "plt.imshow()\n",
    "\n",
    "#This did yield desired results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_img = cv.imread(\"data/4-right.jpeg\") \n",
    "left_img = cv.imread(\"data/4-left.jpeg\")\n",
    "\n",
    "src, dst, result_img = match_descriptors(right_img, left_img)\n",
    "    \n",
    "\n",
    "H, masked = cv.findHomography(src, dst, cv.RANSAC, 5.0)\n",
    "\n",
    "dst = cv.warpPerspective(right_img,H,(left_img.shape[1] + right_img.shape[1], left_img.shape[0]))\n",
    "dst[0:left_img.shape[0], 0:left_img.shape[1]] = left_img\n",
    "cv.imwrite(\"data/4-stitched.jpg\",dst)\n",
    "plt.imshow(dst)\n",
    "plt.imshow()\n",
    "\n",
    "#This did yield the most desired results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    " * Load left image (```data/1-left.jpeg```) and pick at least four points in order.\n",
    " * Load right image (```data/1-right.jpeg```) and pick the corresponding locations.\n",
    " * Use the correspondences to estimate homography.\n",
    " * Use the homography to stitch the images together.\n",
    " \n",
    "### Some outdoor photography\n",
    " * The above image exhibits strong parallex. You may consider going for a walk and taking an image pair of your favorite neighbourhood landscape. Try not to translate the camera, just rotate in place.\n",
    "\n",
    "### Useful information\n",
    " * [Least squares fitting](http://csundergrad.science.uoit.ca/courses/cv-notes/notebooks/15-least-squares.html)\n",
    " * [RANSAC](http://csundergrad.science.uoit.ca/courses/cv-notes/notebooks/17-ransac.html)\n",
    " * [Homography](http://csundergrad.science.uoit.ca/courses/cv-notes/notebooks/19-homography.html)\n",
    " * [Image sampling](http://csundergrad.science.uoit.ca/courses/cv-notes/notebooks/10-image-sampling.html)\n",
    "\n",
    "### Bonus\n",
    " * Try stitching on different image pairs.\n",
    " * Can you stich more than one images to create a panorama?\n",
    "\n",
    "## Submission\n",
    "Include code and stiched image single jupyter notebook. Submit via canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"ontario-tech-univ-logo.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
