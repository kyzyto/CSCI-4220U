{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-d7fb6584c647>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d7fb6584c647>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <center>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"data/ontario-tech-univ-logo.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image filtering\n",
    "\n",
    "Faisal Qureshi\\\n",
    "Professor\\\n",
    "Faculty of Science\\\n",
    "Ontario Tech University\\\n",
    "Oshawa ON Canada\\\n",
    "http://vclab.science.ontariotechu.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outline**\n",
    "\n",
    "\n",
    "*   Image pyramids\n",
    "*   Hybrid images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task**\n",
    "The goal is to laboratory is to construct hybrid images as suggested by Oliva et al. in their Siggraph 2006 paper [doi]. A hybrid image is static images which changes as a function of viewing distance. Check out the following image taken from Wikipedia article on hybrid image. View this image from up close. You will read \"southeast.\" Now move away from your monitor. As you get farther away from your display, it reads \"northwest.\" How can that be? This image combines low-frequency component of one image (that reads \"northwest\") and high-frequency component of the other image (that reads \"southeast\"). Human visual system picks up high-frequency component up close and is unable to pick high-frequency components from a distance. Therefore, from a distance the text reads \"northwest.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"data/south-east.png\" width=\"25%\"></img></td>\n",
    "     </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't feel like moving away from your desk. You can always \"simulate\" this effect by zooming out the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recipe**\n",
    "Given two images A and B\n",
    "\n",
    "* Construct laplacian pyramid of image A\n",
    "* Construct laplacian pyramid of image B\n",
    "* Construct a new pyramid that takes some levels from A and others from B, recall that different levels of a laplacian pyramid captures signals at different frequencies\n",
    "* Reconstruct the image from this new pyramid\n",
    "* Voila<br>\n",
    "\n",
    "For this to work properly, you will need to fidget how many levels you copy from laplacian pyramid for image A and how many levels you copy from laplacian pyramid for image B. Furthermore, you should pick high-frequency component from images that exhibit higher spatial frequency (i.e., lots of edges and texture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Starter code**\n",
    "Use the code below to get things going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-vo4apon2/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9ccc9eba9274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/dog.bmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/cat1.bmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-vo4apon2/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "img1 = cv.imread('data/dog.bmp')\n",
    "img1 = cv.cvtColor(img1, cv.COLOR_BGR2GRAY)\n",
    "img1 = cv.resize(img1, (2048, 2048))\n",
    "\n",
    "img2 = cv.imread('data/cat1.bmp')\n",
    "img2 = cv.cvtColor(img2, cv.COLOR_BGR2GRAY)\n",
    "img2 = cv.resize(img2, (2048, 2048))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.title('cat')\n",
    "plt.imshow(img1, cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);\n",
    "plt.subplot(122)\n",
    "plt.title('DeCaprio')\n",
    "plt.imshow(img2, cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
